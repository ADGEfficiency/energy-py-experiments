




Be explicit: Describe how your proposed approach addresses climate change, demonstrating an understanding of the application area.
Frame your work: The specific problem and/or data proposed should be contextualized in terms of prior work.
Address the impact: Describe the practical implications of your method in addressing the problem you identify, as well as any relevant societal impacts or potential side-effects. We recommend reading our further guidelines on this aspect here.
Explain the ML: Readers may not be familiar with the exact techniques you are using or may desire further detail.
Justify  the ML: Describe why the ML method involved is needed, and why it is a good match for the problem.
Avoid jargon: Jargon is sometimes unavoidable but should be minimized. Ideal submissions will be accessible both to an ML audience and to experts in other relevant fields, without the need for field-specific knowledge. Feel free to direct readers to accessible overviews or review articles for background, where it is impossible to include context directly.


Our approach addresses climate change by demonstrating practical workflows for getting reinforcement learning agents to learn how to dispatch electric battery storage:

- pretraining with optimal policy data generated by linear programming,
- implementing a parallel battery model in numpy that allows parallelization within a single CPU process,
- tips & tricks around the correct setting of initial battery charge, feature engineering and function approximation.


We choose reinforcement learning because the battery storage problem has access to reward signals (price or carbon) and has a sequential, temporal stucture, suitable for modelling as a Markov Decison Process.

Our reinforcement learning agent of choice is Soft-Actor Critic (SAC) because:
- off policy,
- good performance,
- continuous action space.


By giving access to a limited amount of historical data, the performance of the agent on holdout future data can be estimated.  However, as we also have access to true prices on the holdout data, we can calculate a theoretical maximum for both.

This paper takes this idea one step further.  We can use linear programming to generate optimal trajectories, and use these to initialize the memory buffer of our agent.

We look at the performance of:
- an agent trained only on this dataset,
- an agent that replaces this dataset with new data as it takes actions after pretraining,
- an agent that adds to this dataset with new data as it takes actions after pretraining.


Learnings
- quantile transform of price - rather that log, log scaling, standard, min max
- inital charge at random to get battery moving + learn discharge is good
- charge as a feature


Robustness
- test / train split
